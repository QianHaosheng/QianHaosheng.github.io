<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>李宏毅机器学习 Lecture 3:Image as input</title>
      <link href="/2023/03/09/lhy_ml2022lec3/"/>
      <url>/2023/03/09/lhy_ml2022lec3/</url>
      
        <content type="html"><![CDATA[<h3 id="convolutional-neural-network">Convolutional Neural Network</h3><p>卷积神经网络 CNN 常用于影像的任务上</p><p>以image classification图片分类为例</p><p>在做之前往往会把所有图片rescale成同样的大小</p><p>图片可以看作一个三维的tensor（channel数目RGB、宽、高），数值表示每个Pixel像素的强度，可以把tensor拉直当作网络的输入</p><p>后一层的neuron只连接前一层的部分neuron，一般为一个矩形区域，层数越深，看到的原始feature的范围越大</p><p>相同的pattern可能出现在不同位置 让一些neuron共享参数weight完全一样</p><p>移动步长stride</p><p>超出范围 采用padding方法解决</p><p>经常在卷积后搭配池化操作pooling（常见max pooling、meanpooling等）减少运算量，但不是必须的</p><p>最后把neuron拉直接上全连接层、softmax做分类</p><p><strong>CNN is not invariant to scaling and rotation.</strong></p><p>影像任务需要data augmentation（截取放大、旋转等）</p><h3 id="spatial-transformer">Spatial Transformer</h3><p>通过神经网络学习图片的平移、旋转</p><p><span class="math display">\[\begin{bmatrix}x&#39; \\y&#39;\end{bmatrix}=\begin{bmatrix}a &amp; b \\c &amp; d\end{bmatrix}\begin{bmatrix}x \\y\end{bmatrix}+\begin{bmatrix}e \\f\end{bmatrix}\]</span></p><p>网络输出<span class="math inline">\(a,b,c,d,e,f\)</span>六个参数</p>]]></content>
      
      
      <categories>
          
          <category> 李宏毅机器学习 </category>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>李宏毅机器学习 HW2</title>
      <link href="/2023/03/03/lhy_ml2022hw2/"/>
      <url>/2023/03/03/lhy_ml2022hw2/</url>
      
        <content type="html"><![CDATA[<p>通过Strong <ahref="https://github.com/QianHaosheng/lihongyi_ml2022_hw/blob/main/ML2022Spring_HW2_strong.ipynb">完整代码</a></p><p>修改网络结构，增加Batch Normalization和Dropout <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BasicBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim, output_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(BasicBlock, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.block = nn.Sequential(</span><br><span class="line">            nn.Linear(input_dim, output_dim),</span><br><span class="line">            <span class="comment"># nn.ReLU(),</span></span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">            nn.BatchNorm1d(output_dim),</span><br><span class="line">            nn.Dropout(<span class="number">0.2</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.block(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure></p><p>调整部分参数，合并19个frames，增加epoch以及early_stopping，增加网络层数<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># data prarameters</span></span><br><span class="line">concat_nframes = <span class="number">19</span>              <span class="comment"># the number of frames to concat with, n must be odd (total 2k+1 = n frames)</span></span><br><span class="line">train_ratio = <span class="number">0.8</span>               <span class="comment"># the ratio of data used for training, the rest will be used for validation</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># training parameters</span></span><br><span class="line">seed = <span class="number">0</span>                        <span class="comment"># random seed</span></span><br><span class="line">batch_size = <span class="number">512</span>                <span class="comment"># batch size</span></span><br><span class="line">num_epoch = <span class="number">100</span>                   <span class="comment"># the number of training epoch</span></span><br><span class="line">early_stopping = <span class="number">8</span></span><br><span class="line">learning_rate = <span class="number">0.0001</span>          <span class="comment"># learning rate</span></span><br><span class="line">model_path = <span class="string">&#x27;./model.ckpt&#x27;</span>     <span class="comment"># the path where the checkpoint will be saved</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># model parameters</span></span><br><span class="line">input_dim = <span class="number">39</span> * concat_nframes <span class="comment"># the input dim of the model, you should not change the value</span></span><br><span class="line">hidden_layers = <span class="number">3</span>               <span class="comment"># the number of hidden layers</span></span><br><span class="line">hidden_dim = <span class="number">1024</span>                <span class="comment"># the hidden dim</span></span><br></pre></td></tr></table></figure></p><p>根据验证集结果保存结果最好的模型 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> val_acc &gt; best_acc:</span><br><span class="line">    best_acc = val_acc</span><br><span class="line">    torch.save(model.state_dict(), model_path)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;saving model with acc &#123;:.3f&#125;&#x27;</span>.<span class="built_in">format</span>(best_acc/<span class="built_in">len</span>(val_set)))</span><br><span class="line">    early_stop_count = <span class="number">0</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    early_stop_count += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> early_stop_count &gt;= early_stopping:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Epoch: <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, model not improving, early stopping.&quot;</span>)</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> 李宏毅机器学习 </category>
          
          <category> 实验 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>李宏毅机器学习 Lecture 2:What to do if my network fails to train</title>
      <link href="/2023/03/02/lhy_ml2022lec2/"/>
      <url>/2023/03/02/lhy_ml2022lec2/</url>
      
        <content type="html"><![CDATA[<h3 id="训练模型一般流程">训练模型一般流程</h3><p>先检查训练集loss</p><ul><li>训练集loss大，2种可能<ul><li>模型偏差model bias大，增加模型复杂性</li><li>optimization没做好（可能发生简化的模型比原模型效果好）</li></ul></li><li>训练集loss小，检查测试集loss<ul><li>测试集loss大，2种可能<ul><li><p>过拟合overfitting</p><p>增加训练数据，或数据增强data augmentation（左右翻转、截取图片等）</p><p>增加模型限制，减少模型复杂性（更少的参数parameter，更少的特征feature，提前终止earlystopping，正则化regularization，dropout）</p></li><li><p>训练集和测试集不同分布mismatch</p></li></ul></li><li>测试集loss小（期望结果）</li></ul></li></ul><h3 id="临界点">临界点</h3><p>临界点critical point：局部最低/高local min/max 鞍点saddle point</p><p>泰勒级数近似Taylor Series Approximation</p><p>用附近的<span class="math inline">\(\theta&#39;\)</span>近似表示<spanclass="math inline">\(L(\theta) \approxL(\theta&#39;)+(\theta-\theta&#39;)^{T}g+\frac{1}{2}(\theta-\theta&#39;)^{T}H(\theta-\theta&#39;)\)</span></p><p><span class="math inline">\(g\)</span>表示梯度是个向量，<spanclass="math inline">\(g=\nabla L(\theta&#39;)\)</span> <spanclass="math inline">\(g_{i}=\frac{\partial L(\theta&#39;)}{\partial\theta_{i}}\)</span></p><p><span class="math inline">\(H\)</span>表示Hessian矩阵，<spanclass="math inline">\(H_{ij}=\frac{\partial^{2}}{\partial\theta_{i}\partial\theta_{j}}L(\theta&#39;)\)</span></p><p>在临界点梯度为0，上式中第二项为0，</p><p>对于所有的<span class="math inline">\(v\)</span>，满足<spanclass="math inline">\(v^{T}Hv&gt;0\)</span>，说明<spanclass="math inline">\(L(\theta)&gt;L(\theta&#39;)\)</span>，该点为局部最低（等价于<spanclass="math inline">\(H\)</span>是正定positivedefinite的，所有特征值eigen value为正）</p><p>对于所有的<span class="math inline">\(v\)</span>，满足<spanclass="math inline">\(v^{T}Hv&lt;0\)</span>，说明<spanclass="math inline">\(L(\theta)&lt;L(\theta&#39;)\)</span>，该点为局部最高（等价于<spanclass="math inline">\(H\)</span>是负定negativedefinite的，所有特征值为负）</p><p>部分<span class="math inline">\(v^{T}Hv&gt;0\)</span>，部分<spanclass="math inline">\(v^{T}Hv&lt;0\)</span>，该点为鞍点（特征值有正有负，沿着负特征值的特征向量eigenvector的方向更新参数可以继续降低loss）</p><p>实际操作中，因为参数量巨大，在高维空间中很少会有局部最低点</p><h3 id="batch">batch</h3><p>一个epoch划分多个batch，每个batch单独计算loss更新参数</p><p>一次update的速度，没有并行运算情况下小batch更快，有并行运算情况下小batch和大batch（不能太大）接近</p><p>一个epoch的速度，大batch更快</p><p>大batch的梯度更稳定，小batch梯度含噪声</p><p>理论上小batch在optimization时更优</p><h3 id="momentum">momentum</h3><p>动量momentum（考虑之前移动的方向）</p><p>从<span class="math inline">\(\theta^0\)</span>开始</p><p>移动<span class="math inline">\(m^0=0\)</span></p><p>计算梯度<span class="math inline">\(g^0\)</span></p><p>移动<span class="math inline">\(m^1=\lambda m^0-\eta g^0\)</span></p><p>更新参数<spanclass="math inline">\(\theta^1=\theta^0+m^1\)</span></p><p>计算梯度<span class="math inline">\(g^1\)</span></p><p>移动<span class="math inline">\(m^2=\lambda m^1-\eta g^1\)</span></p><p>更新参数<spanclass="math inline">\(\theta^2=\theta^1+m^2\)</span></p><p>依此类推</p><h3 id="adaptive-learning-rate">adaptive learning rate</h3><p>loss变化小不一定说明梯度很小</p><ul><li><p>Root Mean Square 均方根方法 在Adagrad中使用</p><p><span class="math inline">\(\theta_{i}^{t+1} \leftarrow\theta_{i}^{t}-\frac{\eta}{\sigma_{i}^{t}}g_{i}^{t}\)</span></p><p><span class="math inline">\(\theta_{i}^{1} \leftarrow\theta_{i}^{0}-\frac{\eta}{\sigma_{i}^{0}}g_{i}^{0}\)</span> <spanclass="math inline">\(\sigma_{i}^{0}=\sqrt{(g_{i}^{0})^{2}}\)</span></p><p><span class="math inline">\(\theta_{i}^{2} \leftarrow\theta_{i}^{1}-\frac{\eta}{\sigma_{i}^{1}}g_{i}^{1}\)</span> <spanclass="math inline">\(\sigma_{i}^{1}=\sqrt{\frac{1}{2}[(g_{i}^{0})^{2}+(g_{i}^{1})^{2}]}\)</span></p><p><span class="math inline">\(\theta_{i}^{3} \leftarrow\theta_{i}^{2}-\frac{\eta}{\sigma_{i}^{2}}g_{i}^{2}\)</span> <spanclass="math inline">\(\sigma_{i}^{2}=\sqrt{\frac{1}{3}[(g_{i}^{0})^{2}+(g_{i}^{1})^{2}+(g_{i}^{2})^{2}]}\)</span></p><p><span class="math inline">\(\vdots\)</span></p><p><span class="math inline">\(\theta_{i}^{t+1} \leftarrow\theta_{i}^{t}-\frac{\eta}{\sigma_{i}^{t}}g_{i}^{t}\)</span> <spanclass="math inline">\(\sigma_{i}^{t}=\sqrt{\frac{1}{t+1}\sum_{j=0}^{t}(g_{i}^{j})^{2}}\)</span></p></li><li><p>RMSProp</p><p><span class="math inline">\(\theta_{i}^{t+1} \leftarrow\theta_{i}^{t}-\frac{\eta}{\sigma_{i}^{t}}g_{i}^{t}\)</span></p><p><span class="math inline">\(\theta_{i}^{1} \leftarrow\theta_{i}^{0}-\frac{\eta}{\sigma_{i}^{0}}g_{i}^{0}\)</span> <spanclass="math inline">\(\sigma_{i}^{0}=\sqrt{(g_{i}^{0})^{2}}\)</span></p><p><span class="math inline">\(\theta_{i}^{2} \leftarrow\theta_{i}^{1}-\frac{\eta}{\sigma_{i}^{1}}g_{i}^{1}\)</span> <spanclass="math inline">\(\sigma_{i}^{1}=\sqrt{\alpha(\sigma_{i}^{0})^2+(1-\alpha)(g_{i}^{1})^{2}}\(0 \lt \alpha \lt 1)\)</span></p><p><span class="math inline">\(\theta_{i}^{3} \leftarrow\theta_{i}^{2}-\frac{\eta}{\sigma_{i}^{2}}g_{i}^{2}\)</span> <spanclass="math inline">\(\sigma_{i}^{2}=\sqrt{\alpha(\sigma_{i}^{1})^2+(1-\alpha)(g_{i}^{2})^{2}}\)</span></p><p><span class="math inline">\(\vdots\)</span></p><p><span class="math inline">\(\theta_{i}^{t+1} \leftarrow\theta_{i}^{t}-\frac{\eta}{\sigma_{i}^{t}}g_{i}^{t}\)</span> <spanclass="math inline">\(\sigma_{i}^{t}=\sqrt{\alpha(\sigma_{i}^{t-1})^2+(1-\alpha)(g_{i}^{t})^{2}}\)</span></p></li><li><p>Adam: RMSProp + Momentum</p></li></ul><p>learning rate scheduling <span class="math inline">\(\theta_{i}^{t+1}\leftarrow\theta_{i}^{t}-\frac{\eta^{t}}{\sigma_{i}^{t}}g_{i}^{t}\)</span></p><ul><li>learning rate decay 随时间减少学习率</li><li>warm up 先增加后减少</li></ul><h3 id="batch-normalization">batch normalization</h3><p>feature normalization 特征归一化</p><p>对所有样本的某一维做归一化 <span class="math inline">\(\tilde{x_{i}}\leftarrow \frac{x_{i}-m_{i}}{\sigma_{i}}\)</span></p><p>使得均值为0，方差为1</p><p>同理，在网络内部也可以进行归一化</p><p>如果激活函数为Sigmoid，一般在前面做归一化，如果激活函数为ReLU，一般在后面做归一化，并在Dropout之前</p><p>测试过程中，BN层一般使用训练集的均值和标准差</p>]]></content>
      
      
      <categories>
          
          <category> 李宏毅机器学习 </category>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>李宏毅机器学习 HW1</title>
      <link href="/2023/02/24/lhy_ml2022hw1/"/>
      <url>/2023/02/24/lhy_ml2022hw1/</url>
      
        <content type="html"><![CDATA[<p>修改网络结构，增加层数、Batch Normalization和Dropout<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">My_Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(My_Model, self).__init__()</span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> modify model&#x27;s structure, be aware of dimensions. </span></span><br><span class="line">        self.layers = nn.Sequential(</span><br><span class="line">            nn.Linear(input_dim, <span class="number">32</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">32</span>),</span><br><span class="line">            nn.Dropout(<span class="number">0.1</span>),</span><br><span class="line"></span><br><span class="line">            nn.Linear(<span class="number">32</span>, <span class="number">16</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">16</span>),</span><br><span class="line">            nn.Dropout(<span class="number">0.1</span>),</span><br><span class="line"></span><br><span class="line">            nn.Linear(<span class="number">16</span>, <span class="number">8</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">8</span>),</span><br><span class="line">            nn.Dropout(<span class="number">0.1</span>),</span><br><span class="line"></span><br><span class="line">            nn.Linear(<span class="number">8</span>, <span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.layers(x)</span><br><span class="line">        x = x.squeeze(<span class="number">1</span>) <span class="comment"># (B, 1) -&gt; (B)</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure></p><p>使用sklearn包选择K个最优特征 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> f_regression</span><br><span class="line"></span><br><span class="line">features = pd.read_csv(<span class="string">&#x27;./covid.train.csv&#x27;</span>)</span><br><span class="line">x_data, y_data = features.iloc[:,:-<span class="number">1</span>], features.iloc[:,-<span class="number">1</span>]</span><br><span class="line">k = <span class="number">24</span></span><br><span class="line">selector = SelectKBest(score_func=f_regression, k=k)</span><br><span class="line">result = selector.fit(x_data, y_data)</span><br><span class="line">idx = np.argsort(result.scores_)[::-<span class="number">1</span>]</span><br><span class="line">selected_idx = <span class="built_in">list</span>(np.sort(idx[:k]))</span><br></pre></td></tr></table></figure></p><p>改用Adam优化器做梯度下降，并且增加learning rate scheduler<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">5e-4</span>, weight_decay=<span class="number">1e-4</span>) </span><br><span class="line">scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=<span class="number">2</span>, T_mult=<span class="number">2</span>, eta_min=<span class="number">1e-5</span>)</span><br></pre></td></tr></table></figure></p><p>修改部分参数 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">config = &#123;</span><br><span class="line">    <span class="string">&#x27;seed&#x27;</span>: <span class="number">5201314</span>,      <span class="comment"># Your seed number, you can pick your lucky number. :)</span></span><br><span class="line">    <span class="string">&#x27;select_all&#x27;</span>: <span class="literal">False</span>,   <span class="comment"># Whether to use all features.</span></span><br><span class="line">    <span class="string">&#x27;valid_ratio&#x27;</span>: <span class="number">0.2</span>,   <span class="comment"># validation_size = train_size * valid_ratio</span></span><br><span class="line">    <span class="string">&#x27;n_epochs&#x27;</span>: <span class="number">5000</span>,     <span class="comment"># Number of epochs.            </span></span><br><span class="line">    <span class="string">&#x27;batch_size&#x27;</span>: <span class="number">256</span>, </span><br><span class="line">    <span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">1e-5</span>,              </span><br><span class="line">    <span class="string">&#x27;early_stop&#x27;</span>: <span class="number">500</span>,    <span class="comment"># If model has not improved for this many consecutive epochs, stop training.     </span></span><br><span class="line">    <span class="string">&#x27;save_path&#x27;</span>: <span class="string">&#x27;./models/model.ckpt&#x27;</span>  <span class="comment"># Your model will be saved here.</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>对数据做归一化 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x_mean, x_std = x_train.mean(axis=<span class="number">0</span>), x_train.std(axis=<span class="number">0</span>)</span><br><span class="line">x_train = (x_train - x_mean) / x_std</span><br><span class="line">x_valid = (x_valid - x_mean) / x_std</span><br><span class="line">x_test = (x_test - x_mean) / x_std</span><br></pre></td></tr></table></figure></p><p>通过Strong <ahref="https://github.com/QianHaosheng/lihongyi_ml2022_hw/blob/main/ML2022Spring_HW1_strong.ipynb">完整代码</a></p>]]></content>
      
      
      <categories>
          
          <category> 李宏毅机器学习 </category>
          
          <category> 实验 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch基本操作</title>
      <link href="/2023/02/22/pytorch_basic/"/>
      <url>/2023/02/22/pytorch_basic/</url>
      
        <content type="html"><![CDATA[<h3 id="什么是pytorch">什么是PyTorch</h3><p>一种深度学习框架</p><p>优势</p><ul><li><p>在GPU上进行tensor的计算</p></li><li><p>自动计算梯度</p></li></ul><h3 id="训练一个神经网络的步骤">训练一个神经网络的步骤</h3><ul><li>定义神经网络</li><li>定义损失函数</li><li>设计优化算法</li></ul><h3 id="读数据">读数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data = MyDataset(file)</span><br><span class="line">dataloader = DataLoader(data, batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 一般训练集上shuffle设置为True，测试集上shuffle设置为False</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义自己的Dataset</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, file</span>): <span class="comment"># 读数据+预处理</span></span><br><span class="line">        self.data = ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>): <span class="comment"># 从数据集返回一个样本</span></span><br><span class="line">        <span class="keyword">return</span> self.data[index]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>): <span class="comment"># 返回数据集样本个数</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.data)</span><br></pre></td></tr></table></figure><h3 id="tensor操作">tensor操作</h3><p>.shape() 查看tensor维度</p><p>x = torch.tensor(list) 从list转成tensor</p><p>x = tensor.from_numpy(array) 从numpy.ndarray转成tensor</p><p>x = tensor.zeros(shape) 构造全为0的tensor</p><p>x = tensor.ones(shape) 构造全为1的tensor</p><p>z = x + y 加</p><p>z = x - y 减</p><p>y = x.pow(2) 幂</p><p>y = x.sum() 求和</p><p>y = x.mean() 平均</p><p>x = x.transpose(a, b) 交换维度</p><p>x = x.squeeze(a) 去除一个维度</p><p>x = x.unsqueeze(a) 增加一个维度</p><p>w = torch.cat([x, y], dim=a) 合并tensor</p><p>x = x.reshape(shape) 调整成新的维度</p><p>x = x.to(device) 移动tensor到指定设备</p><h3 id="定义神经网络">定义神经网络</h3><p>nn.Linear(in_features, out_features) 全连接层fully-connectedlayer</p><p>nn.Sigmoid() Sigmoid函数</p><p>nn.ReLU() ReLU函数 还有nn.LeakyReLU(x)等</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>): <span class="comment"># 初始化模型，定义网络结构</span></span><br><span class="line">        <span class="built_in">super</span>(MyModel, self).__init__()</span><br><span class="line">        self.net = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="keyword">in</span>,...),</span><br><span class="line">            ...,</span><br><span class="line">            nn.Linear(...,out),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>): <span class="comment"># 计算模型输出</span></span><br><span class="line">        <span class="keyword">return</span> self.net(x)</span><br></pre></td></tr></table></figure><p>等价于</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>): <span class="comment"># 初始化模型，定义网络结构</span></span><br><span class="line">        <span class="built_in">super</span>(MyModel, self).__init__()</span><br><span class="line">        self.layer1 = nn.Linear(<span class="keyword">in</span>,...),</span><br><span class="line">        ...</span><br><span class="line">        self.layern = nn.Linear(...,out),</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>): <span class="comment"># 计算模型输出</span></span><br><span class="line">        out = self.layer1(x)</span><br><span class="line">        out = self.layer2(out)</span><br><span class="line">        ...</span><br><span class="line">        out = self.layern(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><h3 id="定义损失函数">定义损失函数</h3><p>torch.nn.MSELoss</p><p>torch.nn.CrossEntropyLoss 等</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">criterion = nn.MSELoss()</span><br><span class="line">loss = criterion(model_output, expected_value)</span><br></pre></td></tr></table></figure><h3 id="设计优化算法">设计优化算法</h3><p>torch.optim</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr, momentum=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>基本步骤</p><ul><li>optimizer.zero_grad() 梯度清零</li><li>loss.backward() 计算loss的梯度</li><li>optimizer.step() 调整模型参数</li></ul><h3 id="保存加载模型">保存/加载模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model.state_dict(), path) <span class="comment"># 保存模型</span></span><br><span class="line"></span><br><span class="line">ckpt = torch.load(path) <span class="comment"># 加载模型</span></span><br><span class="line">model.load_state_dict(ckpt)</span><br></pre></td></tr></table></figure><h3 id="其他">其他</h3><p>torch.cuda.is_available() 判断是否有gpu</p><p>model.train() 训练阶段</p><blockquote><p>启用BatchNormalization和Dropout，model.train()保证BN层能够用到每一批数据的均值和方差，对于Dropout，随机取一部分网络连接来训练更新参数。</p></blockquote><p>model.eval() 测试阶段</p><blockquote><p>不启用BatchNormalization和Dropout，model.eval()保证BN层能够用到全部训练数据的均值和方差，即测试过程中保证BN层的均值和方差不变，对于Dropout，利用到了所有网络连接，不随机舍弃神经元。</p></blockquote><p>with torch.no_grad()不进行梯度计算，减少内存消耗，确保测试时不进行梯度计算</p><h3 id="随机种子">随机种子</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 固定随机种子方便复现</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">same_seed</span>(<span class="params">seed</span>): </span><br><span class="line">    torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br><span class="line">    torch.backends.cudnn.benchmark = <span class="literal">False</span></span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    torch.manual_seed(seed)</span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        torch.cuda.manual_seed_all(seed)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 李宏毅机器学习 </category>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>李宏毅机器学习 Lecture 1:Introduction of Deep Learning</title>
      <link href="/2023/02/20/lhy_ml2022lec1/"/>
      <url>/2023/02/20/lhy_ml2022lec1/</url>
      
        <content type="html"><![CDATA[<h3 id="基本概念">基本概念</h3><ul><li><p>什么是机器学习machine learning</p><p>机器自动找一个函数function</p></li><li><p>深度学习deep learning</p><p>类神经网络neural network</p></li><li><p>输入</p><p>向量vector、矩阵matrix、序列sequence</p></li><li><p>输出</p><p>数字（回归regression）、类别（分类classification）、文本text/图片image</p></li></ul><h3 id="课程内容">课程内容</h3><ul><li><p>Lec1-5 监督学习supervised learning</p><p>训练数据有标注label</p></li><li><p>Lec6 生成对抗网络generative adversarial network</p><p>输入输出不需要成对，机器自动寻找关联</p></li><li><p>Lec7 自监督学习self-supervised learning</p><p>预训练pre-train（数据无标注）</p><p>下游任务downstream task 微调fine-tune</p></li><li><p>Lec8 异常检测anomaly detection</p></li><li><p>Lec9 可解释AI explainable AI</p></li><li><p>Lec10 模型攻击model attack</p></li><li><p>Lec11 领域自适应domain adapation</p></li><li><p>Lec12 强化学习reinforcement learning</p><p>不知道如何标注数据，但可以判断好坏</p></li><li><p>Lec13 模型压缩network compression</p></li><li><p>Lec14 终身学习life-long learning</p></li><li><p>Lec15 元学习meta learning</p><p>学习如何学习 few-shot learning 用非常少量的标注资料学习</p></li></ul><h3 id="以线性回归为例">以线性回归为例</h3><ul><li><p>模型Model</p><p><span class="math display">\[y=wx+b\]</span></p></li><li><p>损失函数Loss</p><p><span class="math display">\[L(b,w)=\frac{1}{n}\sum_{i=1}^{n}|y_{i}-\hat{y}_{i}|\]</span></p><p>常用损失</p><p>MAE(mean absolute error)</p><p>MSE(mean square error)</p><p>Cross-entropy</p></li><li><p>优化Optimization</p><p><span class="math display">\[w^{*},b^{*}=\arg\min_{w,b}L\]</span></p><p>梯度下降gradient descent</p><ul><li><p>（随机）初始化参数<spanclass="math inline">\(w^{0},b^{0}\)</span></p></li><li><p>计算<span class="math inline">\(\frac{\partial L}{\partialw}|_{w=w^{0}}\)</span>，<span class="math inline">\(\frac{\partialL}{\partial b}|_{b=b^{0}}\)</span></p></li><li><p>更新<span class="math inline">\(w^{1} \leftarrow w^{0} - \eta\frac{\partial L}{\partial w}|_{w=w^{0}}\)</span>，<spanclass="math inline">\(b^{1} \leftarrow b^{0} - \eta \frac{\partialL}{\partial b}|_{b=b^{0}}\)</span></p></li><li><p>不断重复以上操作更新<spanclass="math inline">\(w,b\)</span></p></li></ul><p>学习率learning rate</p><p>超参数hyperparameter 需要自己设定</p><p>梯度下降存在陷入局部最优问题，但事实上影响不大</p></li></ul><h3 id="神经网络">神经网络</h3><p>任意Piecewise Linear曲线都可以看成一个常数和若干HardSigmoid函数的组合</p><p>亦可以用Sigmoid函数或ReLU函数拟合任意函数（常用的激活函数：Sigmoid、ReLU）</p><p>往上堆叠层数（hidden layer），构成神经网络</p><p>反向传播backpropagation使用链式求导的思想高效率计算梯度下降</p><p>正则化regularization使模型更加平滑，减少数据偏差带来的影响</p>]]></content>
      
      
      <categories>
          
          <category> 李宏毅机器学习 </category>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
