<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>李宏毅机器学习 HW1</title>
      <link href="/2023/02/24/lhy_ml2022hw1/"/>
      <url>/2023/02/24/lhy_ml2022hw1/</url>
      
        <content type="html"><![CDATA[<p>修改网络结构，增加层数、Batch Normalization和Dropout<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">My_Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(My_Model, self).__init__()</span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> modify model&#x27;s structure, be aware of dimensions. </span></span><br><span class="line">        self.layers = nn.Sequential(</span><br><span class="line">            nn.Linear(input_dim, <span class="number">32</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">32</span>),</span><br><span class="line">            nn.Dropout(<span class="number">0.1</span>),</span><br><span class="line"></span><br><span class="line">            nn.Linear(<span class="number">32</span>, <span class="number">16</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">16</span>),</span><br><span class="line">            nn.Dropout(<span class="number">0.1</span>),</span><br><span class="line"></span><br><span class="line">            nn.Linear(<span class="number">16</span>, <span class="number">8</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">8</span>),</span><br><span class="line">            nn.Dropout(<span class="number">0.1</span>),</span><br><span class="line"></span><br><span class="line">            nn.Linear(<span class="number">8</span>, <span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.layers(x)</span><br><span class="line">        x = x.squeeze(<span class="number">1</span>) <span class="comment"># (B, 1) -&gt; (B)</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure></p><p>使用sklearn包选择K个最优特征 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> f_regression</span><br><span class="line"></span><br><span class="line">features = pd.read_csv(<span class="string">&#x27;./covid.train.csv&#x27;</span>)</span><br><span class="line">x_data, y_data = features.iloc[:,:-<span class="number">1</span>], features.iloc[:,-<span class="number">1</span>]</span><br><span class="line">k = <span class="number">24</span></span><br><span class="line">selector = SelectKBest(score_func=f_regression, k=k)</span><br><span class="line">result = selector.fit(x_data, y_data)</span><br><span class="line">idx = np.argsort(result.scores_)[::-<span class="number">1</span>]</span><br><span class="line">selected_idx = <span class="built_in">list</span>(np.sort(idx[:k]))</span><br></pre></td></tr></table></figure></p><p>改用Adam优化器做梯度下降，并且增加learning rate scheduler<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">5e-4</span>, weight_decay=<span class="number">1e-4</span>) </span><br><span class="line">scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=<span class="number">2</span>, T_mult=<span class="number">2</span>, eta_min=<span class="number">1e-5</span>)</span><br></pre></td></tr></table></figure></p><p>修改部分参数 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">config = &#123;</span><br><span class="line">    <span class="string">&#x27;seed&#x27;</span>: <span class="number">5201314</span>,      <span class="comment"># Your seed number, you can pick your lucky number. :)</span></span><br><span class="line">    <span class="string">&#x27;select_all&#x27;</span>: <span class="literal">False</span>,   <span class="comment"># Whether to use all features.</span></span><br><span class="line">    <span class="string">&#x27;valid_ratio&#x27;</span>: <span class="number">0.2</span>,   <span class="comment"># validation_size = train_size * valid_ratio</span></span><br><span class="line">    <span class="string">&#x27;n_epochs&#x27;</span>: <span class="number">5000</span>,     <span class="comment"># Number of epochs.            </span></span><br><span class="line">    <span class="string">&#x27;batch_size&#x27;</span>: <span class="number">256</span>, </span><br><span class="line">    <span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">1e-5</span>,              </span><br><span class="line">    <span class="string">&#x27;early_stop&#x27;</span>: <span class="number">500</span>,    <span class="comment"># If model has not improved for this many consecutive epochs, stop training.     </span></span><br><span class="line">    <span class="string">&#x27;save_path&#x27;</span>: <span class="string">&#x27;./models/model.ckpt&#x27;</span>  <span class="comment"># Your model will be saved here.</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>对数据做归一化 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x_mean, x_std = x_train.mean(axis=<span class="number">0</span>), x_train.std(axis=<span class="number">0</span>)</span><br><span class="line">x_train = (x_train - x_mean) / x_std</span><br><span class="line">x_valid = (x_valid - x_mean) / x_std</span><br><span class="line">x_test = (x_test - x_mean) / x_std</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> 实验 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch基本操作</title>
      <link href="/2023/02/22/pytorch_basic/"/>
      <url>/2023/02/22/pytorch_basic/</url>
      
        <content type="html"><![CDATA[<h3 id="什么是pytorch">什么是PyTorch</h3><p>一种深度学习框架</p><p>优势</p><ul><li><p>在GPU上进行tensor的计算</p></li><li><p>自动计算梯度</p></li></ul><h3 id="训练一个神经网络的步骤">训练一个神经网络的步骤</h3><ul><li>定义神经网络</li><li>定义损失函数</li><li>设计优化算法</li></ul><h3 id="读数据">读数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data = MyDataset(file)</span><br><span class="line">dataloader = DataLoader(data, batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 一般训练集上shuffle设置为True，测试集上shuffle设置为False</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义自己的Dataset</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, file</span>): <span class="comment"># 读数据+预处理</span></span><br><span class="line">        self.data = ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>): <span class="comment"># 从数据集返回一个样本</span></span><br><span class="line">        <span class="keyword">return</span> self.data[index]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>): <span class="comment"># 返回数据集样本个数</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.data)</span><br></pre></td></tr></table></figure><h3 id="tensor操作">tensor操作</h3><p>.shape() 查看tensor维度</p><p>x = torch.tensor(list) 从list转成tensor</p><p>x = tensor.from_numpy(array) 从numpy.ndarray转成tensor</p><p>x = tensor.zeros(shape) 构造全为0的tensor</p><p>x = tensor.ones(shape) 构造全为1的tensor</p><p>z = x + y 加</p><p>z = x - y 减</p><p>y = x.pow(2) 幂</p><p>y = x.sum() 求和</p><p>y = x.mean() 平均</p><p>x = x.transpose(a, b) 交换维度</p><p>x = x.squeeze(a) 去除一个维度</p><p>x = x.unsqueeze(a) 增加一个维度</p><p>w = torch.cat([x, y], dim=a) 合并tensor</p><p>x = x.reshape(shape) 调整成新的维度</p><p>x = x.to(device) 移动tensor到指定设备</p><h3 id="定义神经网络">定义神经网络</h3><p>nn.Linear(in_features, out_features) 全连接层fully-connectedlayer</p><p>nn.Sigmoid() Sigmoid函数</p><p>nn.ReLU() ReLU函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>): <span class="comment"># 初始化模型，定义网络结构</span></span><br><span class="line">        <span class="built_in">super</span>(MyModel, self).__init__()</span><br><span class="line">        self.net = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="keyword">in</span>,...),</span><br><span class="line">            ...,</span><br><span class="line">            nn.Linear(...,out),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>): <span class="comment"># 计算模型输出</span></span><br><span class="line">        <span class="keyword">return</span> self.net(x)</span><br></pre></td></tr></table></figure><p>等价于</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>): <span class="comment"># 初始化模型，定义网络结构</span></span><br><span class="line">        <span class="built_in">super</span>(MyModel, self).__init__()</span><br><span class="line">        self.layer1 = nn.Linear(<span class="keyword">in</span>,...),</span><br><span class="line">        ...</span><br><span class="line">        self.layern = nn.Linear(...,out),</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>): <span class="comment"># 计算模型输出</span></span><br><span class="line">        out = self.layer1(x)</span><br><span class="line">        out = self.layer2(out)</span><br><span class="line">        ...</span><br><span class="line">        out = self.layern(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><h3 id="定义损失函数">定义损失函数</h3><p>torch.nn.MSELoss</p><p>torch.nn.CrossEntropyLoss 等</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">criterion = nn.MSELoss()</span><br><span class="line">loss = criterion(model_output, expected_value)</span><br></pre></td></tr></table></figure><h3 id="设计优化算法">设计优化算法</h3><p>torch.optim</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr, momentum=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>基本步骤</p><ul><li>optimizer.zero_grad() 梯度清零</li><li>loss.backward() 计算loss的梯度</li><li>optimizer.step() 调整模型参数</li></ul><h3 id="保存加载模型">保存/加载模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model.state_dict(), path) <span class="comment"># 保存模型</span></span><br><span class="line"></span><br><span class="line">ckpt = torch.load(path) <span class="comment"># 加载模型</span></span><br><span class="line">model.load_state_dict(ckpt)</span><br></pre></td></tr></table></figure><h3 id="其他">其他</h3><p>torch.cuda.is_available() 判断是否有gpu</p><p>model.train() 训练阶段</p><blockquote><p>启用BatchNormalization和Dropout，model.train()保证BN层能够用到每一批数据的均值和方差，对于Dropout，随机取一部分网络连接来训练更新参数。</p></blockquote><p>model.eval() 测试阶段</p><blockquote><p>不启用BatchNormalization和Dropout，model.eval()保证BN层能够用到全部训练数据的均值和方差，即测试过程中保证BN层的均值和方差不变，对于Dropout，利用到了所有网络连接，不随机舍弃神经元。</p></blockquote><p>with torch.no_grad()不进行梯度计算，减少内存消耗，确保测试时不进行梯度计算</p><h3 id="随机种子">随机种子</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 固定随机种子方便复现</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">same_seed</span>(<span class="params">seed</span>): </span><br><span class="line">    torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br><span class="line">    torch.backends.cudnn.benchmark = <span class="literal">False</span></span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    torch.manual_seed(seed)</span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        torch.cuda.manual_seed_all(seed)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>李宏毅机器学习 Lecture 1:Introduction of Deep Learning</title>
      <link href="/2023/02/20/lhy_ml2022lec1/"/>
      <url>/2023/02/20/lhy_ml2022lec1/</url>
      
        <content type="html"><![CDATA[<h3 id="基本概念">基本概念</h3><ul><li><p>什么是机器学习machine learning</p><p>机器自动找一个函数function</p></li><li><p>深度学习deep learning</p><p>类神经网络neural network</p></li><li><p>输入</p><p>向量vector、矩阵matrix、序列sequence</p></li><li><p>输出</p><p>数字（回归regression）、类别（分类classification）、文本text/图片image</p></li></ul><h3 id="课程内容">课程内容</h3><ul><li><p>Lec1-5 监督学习supervised learning</p><p>训练数据有标注label</p></li><li><p>Lec6 生成对抗网络generative adversarial network</p><p>输入输出不需要成对，机器自动寻找关联</p></li><li><p>Lec7 自监督学习self-supervised learning</p><p>预训练pre-train（数据无标注）</p><p>下游任务downstream task 微调fine-tune</p></li><li><p>Lec8 异常检测anomaly detection</p></li><li><p>Lec9 可解释AI explainable AI</p></li><li><p>Lec10 模型攻击model attack</p></li><li><p>Lec11 领域自适应domain adapation</p></li><li><p>Lec12 强化学习reinforcement learning</p><p>不知道如何标注数据，但可以判断好坏</p></li><li><p>Lec13 模型压缩network compression</p></li><li><p>Lec14 终身学习life-long learning</p></li><li><p>Lec15 元学习meta learning</p><p>学习如何学习 few-shot learning 用非常少量的标注资料学习</p></li></ul><h3 id="以线性回归为例">以线性回归为例</h3><ul><li><p>模型Model</p><p><span class="math display">\[y=wx+b\]</span></p></li><li><p>损失函数Loss</p><p><span class="math display">\[L(b,w)=\frac{1}{n}\sum_{i=1}^{n}|y_{i}-\hat{y}_{i}|\]</span></p><p>常用损失</p><p>MAE(mean absolute error)</p><p>MSE(mean square error)</p><p>Cross-entropy</p></li><li><p>优化Optimization</p><p><span class="math display">\[w^{*},b^{*}=\arg\min_{w,b}L\]</span></p><p>梯度下降gradient descent</p><ul><li><p>（随机）初始化参数<spanclass="math inline">\(w^{0},b^{0}\)</span></p></li><li><p>计算<span class="math inline">\(\frac{\partial L}{\partialw}|_{w=w^{0}}\)</span>，<span class="math inline">\(\frac{\partialL}{\partial b}|_{b=b^{0}}\)</span></p></li><li><p>更新<span class="math inline">\(w^{1} \leftarrow w^{0} - \eta\frac{\partial L}{\partial w}|_{w=w^{0}}\)</span>，<spanclass="math inline">\(b^{1} \leftarrow b^{0} - \eta \frac{\partialL}{\partial b}|_{b=b^{0}}\)</span></p></li><li><p>不断重复以上操作更新<spanclass="math inline">\(w,b\)</span></p></li></ul><p>学习率learning rate</p><p>超参数hyperparameter 需要自己设定</p><p>梯度下降存在陷入局部最优问题，但事实上影响不大</p></li></ul><h3 id="神经网络">神经网络</h3><p>任意Piecewise Linear曲线都可以看成一个常数和若干HardSigmoid函数的组合</p><p>亦可以用Sigmoid函数或ReLU函数拟合任意函数（常用的激活函数：Sigmoid、ReLU）</p><p>往上堆叠层数（hidden layer），构成神经网络</p>]]></content>
      
      
      <categories>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
